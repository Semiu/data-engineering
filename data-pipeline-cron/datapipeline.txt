Real-world examples:
Automated marketing emails
Real-time pricing in rideshare apps
Targeted advertising based on browsing history

The data pipeline, described in the code, is the series of steps in processing data. Some steps are sequential by design, and others can be parallel. 

Intro

Prerequisite knowledge

Environment and tools

Intro to Data Pipelines 
Data pipelines can be represented as Directed Acyclic Graphs (DAGs) - a special subset of graphs in which the edges between the nodes have a specific direction, and no cycle exists. It means nodes can't create a path back to themselves. Nodes are steps in the data pipeline process. Edges are dependencies or relationships. 

	Data validation - ensures the data is present, correct and meaningful. it is critical to ensure that data quality is ensured through automated validation checks while building data pipelines at any organization.

	How Apache Airflow used DAGs - Airflow (an open source tool which structures data pipelines as DAGs) allows users to write DAGs in Python. It is a platform to programmatically author, schedule, and monitor workflows as DAGs.

	How Airflow works - the airflow scheduler executes tasks on various workers  while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative."
Airflow consists of five (5) runtime components: web server, meta store db, scheduler, queue, and worker.
Scheduler is for tracking the progress in DAGS. Orchestrating execution of jobs on a trigger or schedule.
Work Queue holds the state of running DAGs and tasks.
Work processes execute the operations defined in each DAGs.
Metastore db stores credentials, connections, history and configuration
	Schedule in Airflow
	Operators and Tasks in Airflow - Callables and Decorators

Context and Templating in Airflow - Airflow leverages templating to allow users to fill in the blank with important runtime variables for tasks.

Notably, Airflow does not process data in memory. Implying that you cannot pass data from one task to another task using DAG. The output must be written to an external data store. DAG is a collection of nodes and edges that describe the order of operations for a data pipeline. Task is an instatiated step in a pipeline fully parameterized for execution. Operator is an abstract building block that can be configured to perform some work.
In Aiffloe DAGs, Nodes = Tasks, Edges = ordering and dependencies between tasks. Task dependencies can be described using >> and << or `set_upstream` and `set_upstream`.
a >> b means a comes before b. b << a means b comes after a.
a.set_upstream(b) means a comes after b. a.set_downstream(b) means a comes before b.

Connection can be made via Airflow hooks. Hooks give reusable interface to external systems and databases. They take the responsibility of how and where to store your connection strings and secrets off your table. There are many hooks that Airflow can use for integrating into common systems. Examples are: HttpHook, PostgresHook, MysQlhook, etc.