Data Lakes
Data lakes store all the raw data as uploaded from different sources. Stores all data structures. Cost-effective but difficult to analyze. Requires an up-to-date data catalog

The data catalog is a source of truth that compensates for the lack of structure of the data. It records answers to questions like: What is the source of the data, 
where it is being used, who is the owner of the data, and how often is it updated? Having one in data governance is good practice - it helps manage availability, 
confidentiality, and integrity.

Data catalog ensures reproducibility and prevents data lakes from becoming data swamps. Serves as a source of information for reliability, autonomy, scalability, and speed.

Data lake Architecture

This provides the best way to arrange the data lake using AWS data lake formation.
PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/install.html)

Big Data Ecosystems, Spark, and Data Lakes
The modern big data ecosystem is a technological response to the evolution of distributed data processing to handle the growing volume of data. This started with distributed
file storage system (Hadoop HDFS), then massive parallel processing (MapReduce), and lately, Apache Spark. Hadoop + Spark led to the development of data lakes 
to process a large amount of structured and unstructured data.

The current development is lakehouse which is an amalgamation of a data lake (distributed file storage for structured, semi-structured and unstructured data), Spark (massively 
parallel processing technology), and data analysis use cases (such as BI, Reports, Data Science, and Machine Learning). Lakehouse combines the strengths of both data lakes and data warehouses.

Hadoop and MapReduce
Hadoop ecosystem is attached to the boom of big data. Hadoop HDFS, MapReduce, YARN resource manager which schedules the computation resources for users are the components of the Hadoop ecosystem.

MapReduce is a programming technique for manipulating large data sets. It starts by dividing the large dataset for distribution across the data cluster.
In map reduce, data is organized into (key, value) pairs. The shuffle step finds all of the data across the clusters that have the same key. 
And all of those data points with that key are brought into the same network node for further analysis.
At the last stage, the values 

Hadoop and Spark evolved the data warehouse into the data lake. 
Data warehouses are based on explicit and specific data structures, structured data poised to ELT to serve BI, reports, etc. They do not perform well with unstructured data.
Data lakes, on the other hand, are capable of ingesting the massive amount of unstructured and structured data, having Spark providing processing strength for the dataset. Data lakes, unfortunately, cannot support transactions and perform poorly on changing datasets. Data governance is also difficult due to its unstructured nature.
To address this, modern lake houses combine the strengths of data warehouses and data lakes into a single architecture.

How is Spark related to Hadoop?
Spark contains libraries for data analysis, machine learning, graph analysis and streaming live data. Faster than Hadoop because 
it writes into memory rather than to disk like Hadoop.

Data streaming - 
(a) Spark (https://spark.apache.org/docs/latest/streaming-programming-guide.html), an extension of the Spark API for streaming data.

(b) Apache Flink (https://flink.apache.org/)
(c) Apache Storm (https://storm.apache.org/)

Data Lakes - Features and Lakehouse architecture
Though data lakes were improvements from data warehouses, it led to some weaknesses and in the course of ingesting large amount of unstructured data, we lost atomic transaction (failed production jobs left data in a corrupted state), quality enforcement (inconsistent and unusable data) and consistency in data structures (impossible to 
stream and batch process while ingesting). This leads to new solution in the Lakehouse architecture.

Lakehouse architecture's core contribution is the addition of metadata and a governance layer on top of delta lake which is the ETL for any or all types of data (structured, semi-structured and unstructured). This gives a pool of raw data and curated set of data, into three data quality levels: Raw ingestion and history (Bronze), Filtered, cleaned, augmented (Silver), and business level aggregates (gold). This gold level can be supplied to streaming analytics, AI and reporting, and any other downstream usages.

Data lakes in AWS
Data lakes are naturally occurring data stores in an organization. This can be unstructured in texts, multimedia, etc., semi-structured, like CSV, JSON, and the structured, in RDBMS tables.
The old time SFTP (secure file transfer protocol) serve is an example of data lakes. In AWS, S3 buckets are used for data lakes. Data lakes can be used as landing zone for unprocessed data, staging zone for semi-processed data, and rest zone, for processed data. Or as deemed fit. This is not a cut and dry rule in data movement categorization.

Data lakes are technological response to data evolution. Just as HDFS came to solve the challenge of saving file that could not fit into a single storage service, S3 as the AWS service that supports the data lake phenomenon, helps in file storage systems with little cost involved.

Data Privacy in Data Lakes
These are hazards for privacy enforcement because many sources containing PII. Because these sources have wide open access, they should not be directly linked to analytics without proper treatment. The opt-in flags in customer records, for example, is a way to filter the customer record.

Data Curation
Preparing high quality data for others to use. Different zones are staged and made to be processed for to achieve high quality, filtered for privacy, composition from multiple sources.




