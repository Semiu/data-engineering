Amazon Redshift Data Warehouse Solution: Data warehousing requires a database. Amazon RDS is a cloud-based data warehousing solution that can be managed using SQL tools. 
	- Redshift Architecture: Redshift clusters - How they are organized into slices of compute
Redshift is a cluster machine. It has one leader node and 1 to n compute nodes. The leader node interacts with the outside world. But it cordinates the works of the compute nodes.
Each node is an EC2 instance. Therefore, the total number of number of nodes in a Redshift cluster is the number of AWS Ec2 instances used in the cluster.
Each slice in the cluster has dedicated storage and memory, with at least 1 CPU.
The total number of slices in a cluster is the unit of parallelism (that is, the number of partitions that the job will result in) and is the sum of all the slices on the cluster. 	

	- How is RedShift different from Relational DB - Row vs. Column
Redshift is a column-oriented RDBMS. It means: it is best suited for OLAP workloads. 
In traditional RDBMS, multiple queries are executed in parallel. Each query on a single CPU. This is suitable for OLTP applications.
For data warehousing solution which deals with massive data like Amazon Redshift. These are MPP - massively parallel processing. They execute one query on multiple CPUs 
in parallel - on multiple machines. Table is partitioned into small partitions and distributed into different CPUs.


ETL in Amazon Redshift
	- SQL to SQL ETL in AWS Redshift
When copying the result, for example, of a star schema (which was a result of transforming a 3NF tables), into another database server, such as Redshift, then the INSERT 
row statement would be slow. Use the COPY command instead.
Solving SQL to SQL ETL starts with having an ETL server in between the source database and the destination data warehouse.
An ETL server runs a SELECT query on the source DB server. The results are stored in CSV files. Then, INSERT/COPY of the result to the destination DB server.
Amazon EC2 can be used as an ETL server. Then, you can have data copied from AWS as csv files to s3 buckets. And, then another program to copy it from the s3 bucket to Redshift.

The S3 Staging can temporarily store the data copied from the source before they are transitioned/copied to the destination server. 
Ingesting files in parallel to scale - common prefix and Manifest file. Files can be compressed and same AWS region should be used. Also, a table can be split to multiple files
before ingestion so that each Redshift slice will act as a separate worker and will use ingest the split of a file in parallel. So, the process will complete much faster.

Optimizing Redshift Table Design
Optimizing how a table is partitioned up to many pieces to be distributed across slices on different machines
	- Distribution Strategies
		* Distribution Styles: EVEN distribution, ALL distribution, AUTO distribution, KEY distribution
	
	EVEN distribution: If there is a table with a primary key and an FK referencing a dimension. The Load compute evenly distributes (for example, 1000 rows would be evenly
	distributed to 250 per slice in a 4 slice compute). However, to join 2 tables which are distributed using EVEN strategy would be slow because the record would need to be
	shuffled before being put together for the JOIN result. For example, given key (say key=2532) of table 1 will not be on the same slice as the corresponding record in 
	table 2, so record will be copied (shuffled) between slices on different nodes, which results in slow performance.

	ALL distribution: This is is mostly for dimension tables. It is also known as BROADCASTING by replicating a table on all slices. Small tables could be replicated on all
	slices to speed up JOINS.
	
	AUTO distribution: This leaves the decision to RedShift. This is better for small tables. It chooses the small tables better suited for broadcasting. But the large tables
	are distributed using EVEN strategy.

	KEY distribution: Rows having similar values (FK value) are placed in the same slice. The groups are made on partitions. Tables are created with the `distkey`, `sortkey`
	on the main table and both the `distkey`, `sortkey` are used as PK on the table part  

		* Sorting Keys
	Define a column as a sort key. When this is done, rows are sorted before distribution to slices when the data is loaded.

	Distribution and sort keys have generally been proven as a performance-improving strategy for query
