{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3751fb11",
   "metadata": {},
   "source": [
    "In distributed systems, programs should not rely on resources created by previous executions.\n",
    "\n",
    "Idempotent programs do not rely on existing state before starting and can run multiple times without effect on the result.\n",
    "\n",
    "The code below is an example of a program that relies on a previous state\n",
    "\n",
    "`ledgerBalance = getLedgerBalanceFromLastRun()`\n",
    "\n",
    "`ledgerBalance = ledgerBalance + getLedgerBalanceSinceLastRun()`\n",
    "\n",
    "\n",
    "Avoiding the prior state would be by writing like:\n",
    "\n",
    "`ledgerBalance = addAllTransactions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015a15c",
   "metadata": {},
   "source": [
    "The idempotent code allows data to be processed in parrallel. The same code can be called in different threads on different nodes and servers for each block of data. Each program has no reliance on prior execution, therefore, there is no problem splitting up the processing.\n",
    "\n",
    "In writing Spark code, data is not read into regular list or array because the amount of data can be very large to be read to memory. Instead, special datasets such as *Resilient Distributed Datasets (RDDs)* and *Dataframes* are used. These special datasets act like SQL query cursor because they do not hold all the data in memory. \n",
    "\n",
    "These datasets give Spark job access to shared resources of clusters in a controlled way managed outside the Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a897e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of doing something like this \n",
    "\n",
    "textFile = open(\"invoices.txt\", \"r\")\n",
    "\n",
    "# invoiceList could occupy Gigabytes of Memory\n",
    "invoiceList = textFile.readlines()\n",
    "\n",
    "print(invoiceList)\n",
    "\n",
    "# Do something like this instead\n",
    "\n",
    "invoiceDataFrame = spark.read.text(\"invoices.txt\")\n",
    "\n",
    "# Leverage Spark DataFrames to handle large datasets\n",
    "invoiceDataFrame.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b378004",
   "metadata": {},
   "source": [
    "#### Directed Acyclic Graph (DAG)\n",
    "\n",
    "Similar to how cyclists carry their own water, every Spark program makes a copy of its input data and never changes the original parent data. Because Spark doesn't change or mutate the input data, it's known as immutable. But what happens when you have lots of function calls in your program?\n",
    "\n",
    "In Spark, you do this by chaining together multiple function calls that each accomplish a small chunk of the work.\n",
    "It may appear in your code that every step will run sequentially. However, they may be run more efficiently if Spark finds a more optimal execution plan.\n",
    "\n",
    "Spark uses a programming concept called **lazy evaluation.** Before Spark does anything with the data in your program, it first builds step-by-step directions of what functions and data it will need.\n",
    "\n",
    "In Spark, and in other similar computational processes, this is called a Directed Acyclic Graph (DAG). \n",
    "\n",
    "DAG is a progam's path of execution that avoids explicit repetition. \n",
    "\n",
    "For example, if a specific file is read more than once in your code, Spark will only read it one time. Spark builds the DAG from your code, and checks if it can procrastinate, waiting until the last possible moment to get the data.\n",
    "\n",
    "A cycling team rotates their position in shifts to preserve energy. In Spark, these shifts are called **stages.** \n",
    "\n",
    "\n",
    "As you watch the output of your Spark code, you will see output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583aa33",
   "metadata": {},
   "source": [
    "`[Stage 19:> ======>                                         (0 + 1) / 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5e08a",
   "metadata": {},
   "source": [
    "[Spark Overview](https://spark.apache.org/docs/latest/)\n",
    "\n",
    "[Cluster Mode](https://spark.apache.org/docs/3.0.2/cluster-overview.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692ae4c",
   "metadata": {},
   "source": [
    "Python and SQL when used for data retrieval are run through a query optimizer. Spark converts the optimized query into an execution plan (DAG). The code in the DAG generates RDD.\n",
    "\n",
    "RDD is what is needed to be used in programming Spark in the earlier versions (<1.3). In 1.3, it becomes DataFrame API, and in version 2.0. it is DataFrame and DatasetsAPI.\n",
    "\n",
    "Data sources can be database, CSV, JSON or text files. Spark converts this RDD for efficient data processing. RDD is fault-tolerant, cacheable and partitioned.\n",
    "\n",
    "Working with RDDs might be important when we need to deal with lower level of abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120e50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecae986d",
   "metadata": {},
   "source": [
    "[RDD programming](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "[RDD vs Dataframes vs Datasets](https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee4d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab741a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
