{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe80681a",
   "metadata": {},
   "source": [
    "#### The Spark Session\n",
    "\n",
    "`SparkContext` is the first component and the main entry for Spark functionality for connecting the cluster with the application.\n",
    "\n",
    "To create `SparkContent`, we need `SparkConf` object to specify some parameters, including the master's node IP address.\n",
    "\n",
    "To read data frames, we beed Spark SQL equivalent, the `SparkSession`, which similarly needs some parameters to be specified.\n",
    "\n",
    "Here, the `log_of_songs` is an example of thousands of strings of song that is not passed into the `parallelize` method of the `spark.sparkContext` object.\n",
    "\n",
    "`distributed_song_log_rdd = spark.sparkContext.parallelize(log_of_songs)`\n",
    "\n",
    "So, if processing like `.lower()` is intended to be applied to this, we can have:\n",
    "\n",
    "```bash \n",
    "def convert_to_lower_case(song):\n",
    "    return song.lower()\n",
    "```\n",
    "        \n",
    "We can now use the Spark function `map` to apply the `convert_to_lower_case` function to every song, like:\n",
    "\n",
    "`distributed_song_log_rdd.map(convert_to_lower_case)`\n",
    "\n",
    "This can also be written using the anonymous function `lambda`:\n",
    "\n",
    "`distributed_song_log_rdd.map(lambda song: song.lower())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f776124",
   "metadata": {},
   "source": [
    "#### Distributed Data stores\n",
    "\n",
    "Large data needs distributed computing to be stored. Distributed file systems and databases store  data in a falut-tolerant ways so that when a machine breaks or becomes unavailable, the data is not lost. \n",
    "\n",
    "Hadoop has [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) for its data storage. It splits files into 64 or 128 megabyte blocks and these are replicated across cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f09259",
   "metadata": {},
   "source": [
    "#### Imperative vs Declarative Programming\n",
    "\n",
    "Imperative - which could be Spark DataFrames and Python is about \"How?\" and Declarative programming, SQL, for example, is about \"what?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014aebe",
   "metadata": {},
   "source": [
    "#### Data Wrangling with DataFrames Extra Tips\n",
    "\n",
    "Extra Tips for Working With PySpark DataFrame Functions\n",
    "\n",
    "\n",
    "General functions\n",
    "\n",
    "We have used the following general functions that are quite similar to methods of pandas dataframes:\n",
    "\n",
    "`select()`: returns a new DataFrame with the selected columns\n",
    "\n",
    "`filter()`: filters rows using the given condition\n",
    "\n",
    "`where()`: is just an alias for `filter()`\n",
    "\n",
    "`groupBy()`: groups the DataFrame using the specified columns, so we can run aggregation on them\n",
    "\n",
    "`sort()`: returns a new DataFrame sorted by the specified column(s). By default the second parameter 'ascending' is True.\n",
    "\n",
    "`dropDuplicates()`: returns a new DataFrame with unique rows based on all or just a subset of columns\n",
    "\n",
    "`withColumn()`: returns a new DataFrame by adding a column or replacing the existing column that has the same name. The first parameter is the name of the new column, the second is an expression of how to compute it.\n",
    "\n",
    "Aggregate functions\n",
    "\n",
    "Spark SQL provides built-in methods for the most common aggregations such as `count()`, `countDistinct()`, `avg()`, `max()`, `min()`, etc. in the `pyspark.sql.functions module`. These methods are not the same as the built-in methods in the Python Standard Library, where we can find `min()` for example as well, hence you need to be careful not to use them interchangeably.\n",
    "\n",
    "In many cases, there are multiple ways to express the same aggregations. For example, if we would like to compute one type of aggregate for one or more columns of the DataFrame we can just simply chain the aggregate method after a `groupBy()`. If we would like to use different functions on different columns, `agg()` comes in handy. For example `agg({\"salary\": \"avg\", \"age\": \"max\"})` computes the average salary and maximum age.\n",
    "\n",
    "User defined functions (UDF)\n",
    "\n",
    "In Spark SQL we can define our own functions with the udf method from the `pyspark.sql.functions` module. The default type of the returned variable for UDFs is string. If we would like to return an other type we need to explicitly do so by using the different types from the `pyspark.sql.types` module.\n",
    "\n",
    "Window functions\n",
    "\n",
    "Window functions are a way of combining the values of ranges of rows in a DataFrame. When defining the window we can choose how to sort and group (with the `partitionBy` method) the rows and how wide of a window we'd like to use (described by rangeBetween or rowsBetween).\n",
    "\n",
    "[Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html), [Spark Python API](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1245a30",
   "metadata": {},
   "source": [
    "#### Spark SQL\n",
    "\n",
    "Spark allows querying the the dataframes using SQL code, similar to what can be used in MySQL or PostgresQL \n",
    "\n",
    "* [Spark Built in function](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "\n",
    "* [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-getting-started.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982480f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4839418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
