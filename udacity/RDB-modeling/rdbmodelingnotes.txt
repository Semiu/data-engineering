Database is the physical arrangemement of data. Access to database is always provided by database management system - computer software
that allowsusers to interact with the databases and provide access to the data.

Information in a relational database is represented at the logical level in exactly one way, that is, by values in tables.
This is the rule 1 (information rule) of the Codd's rules. This is the rule expected to be achieved by data modeling.

Importance of relational database modeling are (a) helping in standardization of data model (transforming data into rows
and columns for seamless query using SQL), (b) flexivility in adding and altering tables, (c) data integrity (ensuring
data inputted meets the quality checks and types), (d) SQL - helps to access the data in a standard predefined language, 
(e) Simplicity - data stored in a tabular format, (f) intuitive organization - see spreasheet data organization.

OLAP: Read heavy, for complex analytics and ad hoc queries for aggregations.

OLTP: Support read, insert, update and delete. Little aggregations if any.

Normalization - to reduce data redundancy (copies of data) and increase data integrity (response from the database must
be correct). This is the process of structuring relational database in accordance with series of normal forms in order to
reduce data redundancy. Normalization achieves avoidance of unranted insertions, redunction of refactoring, informative 
relational model, neutral database. This is a step-by-step process.
Though there are up to 6 NF, most DB modeling strives to achieve 3 NF.

First Normal form (1NF) - atomic values in decoupled tables addressing specific relation instead of having everything in a
giant table.
Second Normal form (2NF) - Achieve 1NF. All columns rely on PK
third Normal form (3NF) - Achieve 2NF. No transitive dependencies - meaning when a retrieving information of a column
depends on two other columns in a way that the output might not even be unique. Break into tables in a way that avoids
having duplicate data. We want A->C without passing through B.

Denormalization - for read heavy workloads to increase performance. There would be duplicates of the data because they are
focussed on the queries that would be run. This is a process of improving the read performance of a database at the
expense of some write performance by adding redundant copies of data.
For examples, while breaking down tables into smaller ones to use JOIN allows flexibility and data integrity, but it
 is extremely slow.

Denormalization follows normalization. Meaning tables are denormalized after they have been normalized. Reads (SELECT) 
will be fasterand writes (insert, update, delete) will be slower. In other words, denormalization is all about performance.
Duplicate data is allows for denormalized tables.Denormalization is all about queries. When needing to query with JOIN 
on many tables, there would be less performance. It is the go-to choice when the workload is heavily READ.

With denormalization, we want to examine the queries to be run and find a way of reducing the number of JOIN to be involved.
We want to get the information needed from minimum number of tables possible. Denormalizing normalized tables may lead to
duplicate data. But that is a reasonable tradeoffs.

Denormalization vs. Normalization - Normalization is about increasing data integrity by reducing the number of copies of
data. Data updates (WRITE operations - INSERT, UPDATE, DELETE) would be done in as few places as possible. 
Denormalization is about increasing the performance by reducing the number of JOINS between tables. This would affect 
data integrity anyway.

Facts/Dimension tables - work together for an organized data model. These are created differently in the database. They
are conceptual, and extremely important. They help in understanding Star and Snowflake schemas. Facts consist of metrics,
measurements and facts. Dimensions are updated, while facts take key (FK) referencing the dimension table PK. They contain
all other info nor included in the fact table.

Different schema models - The two popular data mart schema for data wearehousing are (i) star and (ii) snowflake.
Star schema is the simplest - consists of one or more fact tables, referencing number of dimension tables. Star schemas
are denormalized, simplifies queries, fast aggregations. That comes with the drawbacks of denormalization, data integrity.

Snowflake schema - logical arrangement of tables in a multidimensional database represented by centralized fact tables,
connected to multiple dimensions. This is a result of elaborated dimensions. More normalized than star schema but only
 in 1NF or 2NF.

Snowflake vs Star - Star schema is a special and simplified case of snowflake. It does not allow one to many relationships
while snowflake schema does. 
