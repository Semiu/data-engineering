{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iCA-NLPTask-Solution1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import the neccesary libraries \n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "_Mys4Pb9iDVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the CSV data file - mounted from my Google Drive\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Task_data (1) (3).csv\""
      ],
      "metadata": {
        "id": "DBczriJziDSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert the CSV file to a dataframe. It takes the path to the data file as parameter and returns a dataframe of the data\n",
        "\n",
        "def csv_to_df(path):\n",
        "  datadf = pd.read_csv(path, low_memory=False)\n",
        "  return datadf"
      ],
      "metadata": {
        "id": "w2LqPzkRiDPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_description = \"the customer reported that the vela ventilator alarmed low pip (peak inspiratory pressure), low ve (minute ventilation) , xdcr (transducer) fault. the customer confirmed that there was no patient involvement associated with the reported event. vyaire medical file identification: (b)(4). the customer reported that they will not return the defective pcb for evaluation. therefore, no root cause could be determined . vyaire medical will submit a supplemental report in accordance with 21 cfr section 803.56 if additional information was received.\""
      ],
      "metadata": {
        "id": "uQ1wsOMvvulb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keyword_corpus(text_desr):\n",
        "\n",
        "  device_problem = []\n",
        "  patient_problem = []\n",
        "  other_problem = []\n",
        "  \n",
        "  global final_device_problem_keyphrases, final_patient_problem_keyphrases, final_other_problem_keyphrases\n",
        "  \n",
        "  text_list = text_desr.split('.')\n",
        "  \n",
        "  for text in text_list:\n",
        "    if bool(re.search(\"device\", text, re.IGNORECASE)):\n",
        "      device_problem.append(text)\n",
        "    \n",
        "    elif bool(re.search(\"patient\", text, re.IGNORECASE)):\n",
        "      patient_problem.append(text)\n",
        "      \n",
        "    else:\n",
        "      other_problem.append(text)\n",
        "  \n",
        "  if len(device_problem) > 0:\n",
        "    device_problem_keyphrases = [d_problem.split('device')[1] for d_problem in device_problem]\n",
        "    device_problem_keyphrases = [device_problem_keyphrases.split(',') for device_problem_keyphrases in device_problem_keyphrases]\n",
        "    final_device_problem_keyphrases = [item.lstrip() for sublist in device_problem_keyphrases for item in sublist]\n",
        "  \n",
        "  elif len(device_problem) == 0:\n",
        "    final_device_problem_keyphrases = device_problem\n",
        "  \n",
        "  if len(patient_problem) > 0:\n",
        "    patient_problem_keyphrases = [p_problem.split('patient')[1] for p_problem in patient_problem]\n",
        "    patient_problem_keyphrases = [patient_problem_keyphrases.split(',') for patient_problem_keyphrases in patient_problem_keyphrases]\n",
        "    final_patient_problem_keyphrases = [item.lstrip() for sublist in patient_problem_keyphrases for item in sublist]\n",
        "  \n",
        "  elif len(patient_problem) == 0:\n",
        "    final_patient_problem_keyphrases = patient_problem\n",
        "  \n",
        "  if len(other_problem) > 0:\n",
        "    other_problem_keyphrases = [o_problem.split(',') for o_problem in other_problem]\n",
        "    final_other_problem_keyphrases = [item.lstrip() for sublist in other_problem_keyphrases for item in sublist]\n",
        "  \n",
        "  elif len(other_problem) == 0:\n",
        "    final_other_problem_keyphrases = other_problem\n",
        "  \n",
        "  return final_device_problem_keyphrases,  final_patient_problem_keyphrases, final_other_problem_keyphrases"
      ],
      "metadata": {
        "id": "S4fhs_oXihFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WordNetLemmatizer for the NLP processing\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "0eWczx8FTsto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stop words from fundamental English stop words\n",
        "# The stopwords.txt file is a collection of English words that should be eliminated because they do not add any important meaning\n",
        "\n",
        "stopwords = set(w.rstrip() for w in open('/content/drive/MyDrive/stopwords.txt'))\n",
        "\n",
        "# Adding more stopwords specific to this problem - as seen in the raw data\n",
        "# This would be iterately expanded to optimize the performance of the tokenizer\n",
        "\n",
        "stopwords = stopwords.union({'wa','reported'})"
      ],
      "metadata": {
        "id": "IxlpQGbLRy60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Text pre-processing function\n",
        "# This function tokenizes each sentence in the problem list provided, and returns a new list of tokens and any numeric value found \n",
        "\n",
        "def my_tokenizer(sentence):\n",
        "  \n",
        "  eliminate_words = [',',\"'\",'.','(',')','{','}',':','e.g',';', '@', '{}', '()'] # This can also be increased to \n",
        "  sentence = sentence.lower() # downcase\n",
        "  tokens = nltk.tokenize.word_tokenize(sentence) # split string into words (tokens)\n",
        "  tokens = [t for t in tokens if len(t) > 2 or len(t)==1]   # remove short words, they're probably not useful\n",
        "  tokens = [t for t in tokens if t not in eliminate_words] # Remove those weird characters in the eliminate_words list\n",
        "  tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
        "  tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
        "  digits = [t for t in tokens if any(c.isdigit() for c in t)] # get the digits seperately (probably these are the scores)\n",
        "  \n",
        "  return \" \".join(tokens), digits"
      ],
      "metadata": {
        "id": "-Iay3gb4ReS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes the text description as a parameter, passes it to the extract_keyword_corpus function.\n",
        "# The returned values from the extract_keyword_corpus function are further processed to get a dictionary data structure for the required features\n",
        "# This function returns a dictionary where the key-values are the six columns to be populated in the dataframe\n",
        "\n",
        "def extract_keywords_dict(text_desr):\n",
        "  \n",
        "  keyword_corpus = extract_keyword_corpus(text_desr) #The extract_keyword_corpus function is called here\n",
        "\n",
        "  keyword_corpus_device = keyword_corpus[0]\n",
        "  keyword_corpus_patient = keyword_corpus[1]\n",
        "  keyword_corpus_other = keyword_corpus[2]\n",
        "\n",
        "  keywords_dict = {}\n",
        "  \n",
        "  device_keywords = []\n",
        "  device_score = []\n",
        "  patient_keywords = []\n",
        "  patient_score = []\n",
        "  other_keywords = []\n",
        "  other_score = []\n",
        "  \n",
        "  if len(keyword_corpus_device) > 0:\n",
        "    for sent_d in keyword_corpus_device:\n",
        "      sentence_tokens_d = my_tokenizer(sent_d) # The tokenizer function is called here\n",
        "      device_keywords.append(sentence_tokens_d[0])\n",
        "      if len(sentence_tokens_d[1]) > 1:\n",
        "        joined_digits = \" \".join(sentence_tokens_d[1])\n",
        "        device_score.append(joined_digits)\n",
        "      elif len(sentence_tokens_d[1]) == 1:\n",
        "        digits = sentence_tokens_d[1][0]\n",
        "        device_score.append(digits)\n",
        "  \n",
        "  elif len(keyword_corpus_patient) > 0:\n",
        "    for sent_p in keyword_corpus_patient:\n",
        "      sentence_tokens_p = my_tokenizer(sent_p) # The tokenizer function is called here\n",
        "      patient_keywords.append(sentence_tokens_p[0])\n",
        "      if len(sentence_tokens_p[1]) > 1:\n",
        "        joined_digits = \" \".join(sentence_tokens_p[1])\n",
        "        patient_score.append(joined_digits)\n",
        "      elif len(sentence_tokens_p[1]) == 1:\n",
        "        digits = sentence_tokens_p[1][0]\n",
        "        patient_score.append(digits)\n",
        "  \n",
        "  elif len(keyword_corpus_other) > 0:\n",
        "    for sent_o in keyword_corpus_other:\n",
        "      sentence_tokens_o = my_tokenizer(sent_o) # The tokenizer function is called here\n",
        "      other_keywords.append(sentence_tokens_o[0])\n",
        "      if len(sentence_tokens_o[1]) > 1:\n",
        "        joined_digits = \" \".join(sentence_tokens_o[1])\n",
        "        other_score.append(joined_digits)\n",
        "      elif len(sentence_tokens_o[1]) == 1:\n",
        "        digits = sentence_tokens_o[1][0]\n",
        "        other_score.append(digits)\n",
        "  \n",
        "  keywords_dict[\"keywords (Device Problems)\"] = device_keywords\n",
        "  keywords_dict[\"keywords (Device Problems)_score\"] = device_score\n",
        "  keywords_dict[\"keywords (Patient Problems)\"] = patient_keywords\n",
        "  keywords_dict[\"keywords (Patient Problems)_score\"] = patient_score\n",
        "  keywords_dict[\"keywords others (interesting ones)\"] = other_keywords\n",
        "  keywords_dict[\"keywords others (interesting ones)_score\"] = other_score\n",
        "\n",
        "  return keywords_dict"
      ],
      "metadata": {
        "id": "Ljt7TjqLReP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the csv_to_df function that opens the CSV file into a dataframe\n",
        "\n",
        "data_df = csv_to_df(data_path)\n"
      ],
      "metadata": {
        "id": "ejCZHrHPIYD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the process_df function\n",
        "\n",
        "processed_data_df = process_df(data_df)"
      ],
      "metadata": {
        "id": "H_duwBfyW2xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 20 rows of the processed data frame - for the whole data frame\n",
        "\n",
        "processed_data_df.head(20)"
      ],
      "metadata": {
        "id": "RL3ItHZTIYAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed data frame in CSV format - change the file name not to override the unprocessed data - for the whole dataframe\n",
        "\n",
        "def save_processeddf(processed_datadf):\n",
        "  processed_datadf.to_csv(\"/content/drive/MyDrive/processed_Task_data (1) (3).csv\", encoding='utf-8', index=None)"
      ],
      "metadata": {
        "id": "AfaLWDEJ-IaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the save_processeddf function - for the whole dataframe\n",
        "\n",
        "save_processeddf(processed_data_df)"
      ],
      "metadata": {
        "id": "_Dsz0f4rDlGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test script here, using the same texts for solutions 2 and 3 to see the quality of the processed data returned. This processed data \n",
        "# is expected to be used to populate the dataframe's columns requested"
      ],
      "metadata": {
        "id": "V8eRwpm0eDo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = '(b)(4). this reported event and subsequent repairs were investigated through the service repair process. failure data and parts-used information were reviewed for the sap and track wise files and found relevant to the service repair. a review of the device service history record was performed from the date of manufacture to the date corresponding to this service notification number. the database showed no quality notifications were opened for the device. a review of the device history record in sap for sn (b)(4) was performed from the date of the manufacture to date of the release of product, which confirmed that this device was not involved in a production failure, and product was returned for servicing which correlates to the customer reported issue. a trackwise complaint history review was completed, and it was confirmed that there were additional complaints received with similar sn (b)(4) for the same or related failure mode. the customer stated that there was no patient involvement.'"
      ],
      "metadata": {
        "id": "98yVp-xyeDk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"the patient contacted animas alleging that the subject pump auto suspends when he wakes up in the morning. there was no mention of physical damage to the pump. there was no reported patient impact associated with this complaint. (b)(4): all buttons responded to presses normally. the keypad was removed and no damage was found to the button contacts. unrelated to the complaint, evaluation revealed the internal clock battery on the pcb board had failed. the pump would not retain the user programmed date and time settings upon removal of the primary aa battery. when a new aa battery is inserted the pump displays the default date and time which must be manually confirmed (or reset) by the user in order to proceed. the pump has not been returned to animas for evaluation. animas has conducted a review of the device history record for this pump and confirmed that it was operating within required specifications at the time of release. if the device is returned, an evaluation shall be completed and a supplemental report will be filed. no conclusions can be made at this time. (b)(4).\""
      ],
      "metadata": {
        "id": "37TZH_rOeDiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_3 = \"it was reported that the unit was knocked over and subsequently declared multiple errors indicative of a serial peripheral interface failure. there was no patient involvement. the manufacturer's remote service technician performed troubleshooting with the customer. the customer reported that a wire had broken from the battery and the wire was soldered back together. the technician advised the customer against re-soldering components and to replace the battery if damaged. the technician also recommended that the customer replace the data acquisition (da) ribbon cable, followed by the flow sensor and central processing unit (cpu) board. date of event: (b)(6) 2020. date of report: 20apr2020.\""
      ],
      "metadata": {
        "id": "Ir1WC2rJeDY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_4 = \"the service report shows the customer reported that the 840 ventilator stopped cycling while in use on a pt. the pt was not harmed or injured as a result of the event. the nellcor puritan bennett customer support engineer (cse) inspected the device and could not duplicate the alleged event. the unit passed extended self-testing and no parts were replaced. it is not verified that the vent was inoperable, and that a malfunction occurred.\""
      ],
      "metadata": {
        "id": "w0DdCyrjeLXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_5 = \"the customer reported that the vela ventilator alarmed low pip (peak inspiratory pressure), low ve (minute ventilation) , xdcr (transducer) fault. the customer confirmed that there was no patient involvement associated with the reported event. vyaire medical file identification: (b)(4). the customer reported that they will not return the defective pcb for evaluation. therefore, no root cause could be determined . vyaire medical will submit a supplemental report in accordance with 21 cfr section 803.56 if additional information was received.\""
      ],
      "metadata": {
        "id": "cRHWvyQXeLJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keywords_dict(text_1)"
      ],
      "metadata": {
        "id": "oUrFSTtzeLHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf64ef9-140a-4439-ce5d-d69789d22dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': ['service history record performed date manufacture date corresponding this service notification',\n",
              "  '',\n",
              "  'history record sap 4 performed date manufacture date release product',\n",
              "  'confirmed this'],\n",
              " 'keywords (Device Problems)_score': ['4'],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': [],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keywords_dict(text_2)"
      ],
      "metadata": {
        "id": "C608e7QnefTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df77a31-c956-467e-bbe6-c4b265149e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': ['history record this pump confirmed operating required specification time release',\n",
              "  'returned',\n",
              "  'evaluation completed supplemental report filed'],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': [],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keywords_dict(text_3)"
      ],
      "metadata": {
        "id": "g6HsQb_fefEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8c3b14-3fb8-4d63-fb05-93ce7377cd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': [],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': ['involvement'],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': [],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keywords_dict(text_4)"
      ],
      "metadata": {
        "id": "oP0sbgXFee2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61928ae6-f8bc-464a-905b-4c84a0430a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': ['duplicate alleged event'],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': [],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keywords_dict(text_5)"
      ],
      "metadata": {
        "id": "ta9Gli7ueeZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe63627-8d4e-4371-92b2-d8dbdf42049c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': [],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': ['involvement associated event'],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': [],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}