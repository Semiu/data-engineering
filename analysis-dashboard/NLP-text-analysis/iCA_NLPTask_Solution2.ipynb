{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iCA-NLPTask-Solution2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import the neccesary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "cHWj7zA2gIxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the CSV data file - mounted from my Google Drive\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Task_data (1) (3).csv\""
      ],
      "metadata": {
        "id": "OSi3eNBEgIuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert the CSV file to a dataframe. It takes the path to the data file as parameter and returns a dataframe of the data\n",
        "\n",
        "def csv_to_df(path):\n",
        "  datadf = pd.read_csv(path, low_memory=False)\n",
        "  return datadf"
      ],
      "metadata": {
        "id": "AzcuHQoZg_ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create stop words from fundamental English stop words\n",
        "# The stopwords.txt file is a collection of English words that should be eliminated because they do not add any important meaning\n",
        "\n",
        "stopwords = set(w.rstrip() for w in open('/content/drive/MyDrive/stopwords.txt'))\n",
        "\n",
        "# Adding more stopwords specific to this problem - as seen in the raw data\n",
        "# This would be iterately expanded to optimize the performance of the tokenizer\n",
        "\n",
        "stopwords = stopwords.union({'wa','reported'})"
      ],
      "metadata": {
        "id": "3BuOhDBcYLNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes a text description as a parameter and returns three lists which categorise the content of the text description into device\n",
        "# patient, and any other one\n",
        "\n",
        "def extract_keyword_corpus_df(text_desr):\n",
        "\n",
        "  device_problem = []\n",
        "  patient_problem = []\n",
        "  other_problem = []\n",
        "  \n",
        "  global final_device_problem_keyphrases, final_patient_problem_keyphrases, final_other_problem_keyphrases\n",
        "  \n",
        "  text_list = text_desr.split('.')\n",
        "  \n",
        "  for text in text_list:\n",
        "    if bool(re.search(\"device\", text, re.IGNORECASE)):\n",
        "      device_problem.append(text)\n",
        "    \n",
        "    elif bool(re.search(\"patient\", text, re.IGNORECASE)):\n",
        "      patient_problem.append(text)\n",
        "      \n",
        "    else:\n",
        "      other_problem.append(text)\n",
        "  \n",
        "  if len(device_problem) > 0:\n",
        "    device_problem_keyphrases = [d_problem.split('device')[1] for d_problem in device_problem]\n",
        "    device_problem_keyphrases = [device_problem_keyphrases.split(',') for device_problem_keyphrases in device_problem_keyphrases]\n",
        "    final_device_problem_keyphrases = [item.lstrip() for sublist in device_problem_keyphrases for item in sublist]\n",
        "  \n",
        "  elif len(device_problem) == 0:\n",
        "    final_device_problem_keyphrases = device_problem\n",
        "  \n",
        "  if len(patient_problem) > 0:\n",
        "    patient_problem_keyphrases = [p_problem.split('patient')[1] for p_problem in patient_problem]\n",
        "    patient_problem_keyphrases = [patient_problem_keyphrases.split(',') for patient_problem_keyphrases in patient_problem_keyphrases]\n",
        "    final_patient_problem_keyphrases = [item.lstrip() for sublist in patient_problem_keyphrases for item in sublist]\n",
        "  \n",
        "  elif len(patient_problem) == 0:\n",
        "    final_patient_problem_keyphrases = patient_problem\n",
        "  \n",
        "  if len(other_problem) > 0:\n",
        "    other_problem_keyphrases = [o_problem.split(',') for o_problem in other_problem]\n",
        "    final_other_problem_keyphrases = [item.lstrip() for sublist in other_problem_keyphrases for item in sublist]\n",
        "  \n",
        "  elif len(other_problem) == 0:\n",
        "    final_other_problem_keyphrases = other_problem\n",
        "  \n",
        "  return final_device_problem_keyphrases,  final_patient_problem_keyphrases, final_other_problem_keyphrases"
      ],
      "metadata": {
        "id": "04D-vWvV-XUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A tokenizer function, but modified to exclude Lemmatizer and NLTK tokenizer. Instead, split the sentence by white space to get each word of a \n",
        "# sentence as token. It also does not include the digits\n",
        "\n",
        "def modified_tokenizer(sentence):\n",
        "  \n",
        "  eliminate_words = [',',\"'\",'.','(',')','{','}',':','e.g',';', '@', '{}', '()'] # This can also be increased to \n",
        "  sentence = sentence.lower() # downcase\n",
        "  tokens = [t for t in sentence.split(\" \")]\n",
        "  tokens = [t for t in tokens if len(t) > 2 or len(t)==1]   # remove short words, they're probably not useful\n",
        "  tokens = [t for t in tokens if t not in eliminate_words] # Remove those weird characters in the eliminate_words list\n",
        "  tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
        "  #digits = [t for t in tokens if any(c.isdigit() for c in t)] # get the digits seperately (probably these are the scores)\n",
        "  \n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "SciOZ3q-YZ_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The n-gram function\n",
        "\n",
        "def get_ngrams(content, n):\n",
        "  n_gram_output = []\n",
        "  \n",
        "  for i in range(len(content)-n+1):\n",
        "    n_gram_output.append(content[i:i+n])\n",
        "  \n",
        "  return n_gram_output"
      ],
      "metadata": {
        "id": "LTlJS42SYZ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract two or three-word sentences\n",
        "\n",
        "def extract_three_or_two_sen(three_grams):\n",
        "    three_words = []\n",
        "    for ele_ in three_grams:\n",
        "        for ele in ele_:\n",
        "            x = ele.split(' ')\n",
        "            if len(x) == 3 or len(x) == 2:\n",
        "                three_words.append(ele)\n",
        "    return three_words"
      ],
      "metadata": {
        "id": "7pI1roFYeI-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is to give us a corpus of all device, patient and other keywords from the entire dataset\n",
        "\n",
        "def process_keywords_corpus(df):\n",
        "  \n",
        "  keyword_corpus_device_df = []\n",
        "  keyword_corpus_patient_df = []\n",
        "  keyword_corpus_other_df = []\n",
        "\n",
        "  for text in df[\"TEXT DESCRIPTION\"]:\n",
        "    \n",
        "    keyword_corpus_df = extract_keyword_corpus_df(text) #The extract_keyword_corpus function is called here\n",
        "    \n",
        "    if len(keyword_corpus_df[0]) > 0 and keyword_corpus_df[0][0] != \"\":\n",
        "      for sen_d in keyword_corpus_df[0]:\n",
        "        modified_send_d = modified_tokenizer(sen_d)\n",
        "        keyword_corpus_device_df.append(modified_send_d)\n",
        "    \n",
        "    if len(keyword_corpus_df[1]) > 0 and keyword_corpus_df[1][0] != \"\":\n",
        "      for sen_p in keyword_corpus_df[1]:\n",
        "        modified_send_p = modified_tokenizer(sen_p)\n",
        "        keyword_corpus_patient_df.append(modified_send_p)\n",
        "\n",
        "    if len(keyword_corpus_df[2]) > 0 and keyword_corpus_df[2][0] != \"\":\n",
        "      for sen_o in keyword_corpus_df[2]:\n",
        "        modified_send_o = modified_tokenizer(sen_o)\n",
        "        keyword_corpus_other_df.append(modified_send_o)\n",
        "\n",
        "  n = 3\n",
        "\n",
        "  three_two_sen_device = set(extract_three_or_two_sen(get_ngrams(keyword_corpus_device_df, n))) # Function chaining here: extract_three_or_two_sen and get_ngrams functions, with the\n",
        "                                                                                                # respective corpus as parameter\n",
        "  three_two_sen_patient = set(extract_three_or_two_sen(get_ngrams(keyword_corpus_patient_df, n)))\n",
        "  three_two_sen_other = set(extract_three_or_two_sen(get_ngrams(keyword_corpus_other_df, n)))\n",
        "\n",
        "\n",
        "  return three_two_sen_device, three_two_sen_patient, three_two_sen_other"
      ],
      "metadata": {
        "id": "WeR7K74RemFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables for the entire script\n",
        "\n",
        "df = csv_to_df(data_path) # The dataframe is opened\n",
        "keywords_corpus = process_keywords_corpus(df) # The entire corpus from the whole dataset\n",
        "keywords_corpus_device = keywords_corpus[0] # The corpus for device keywords\n",
        "keywords_corpus_patient = keywords_corpus[1] # The corpus for patient keywords\n",
        "keywords_corpus_other = keywords_corpus[2] # The corpus for other keywords"
      ],
      "metadata": {
        "id": "w-yuFf9Z9iHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The function processes each text in the text description. It takes a text string as a parameter and return a dictionary\n",
        "\n",
        "def process_keywordsdict_per_text(text):\n",
        "\n",
        "  keywords_dict = {}\n",
        "  \n",
        "  device_keywords = []\n",
        "  device_score = [] # Just initialized, but not populated\n",
        "  patient_keywords = []\n",
        "  patient_score = [] # Just initialized, but not populated\n",
        "  other_keywords = []\n",
        "  other_score = [] # Just initialized, but not populated\n",
        "\n",
        "  keywords_per_text = extract_keyword_corpus_df(text) # The extract_keyword_corpus function is called here. The same function for the keyword corpus. \n",
        "                                                      # This time, however, it is called per text\n",
        "  keywords_device = keywords_per_text[0]\n",
        "  keywords_patient = keywords_per_text[1]\n",
        "  keywords_other = keywords_per_text[2]\n",
        "\n",
        "  if len(keywords_device) > 0:\n",
        "    for sent_d in keywords_device:\n",
        "      sentence_tokens_d = modified_tokenizer(sent_d) # The modified tokenizer function is called here\n",
        "      \n",
        "      if sentence_tokens_d in keywords_corpus_device: # To stick to two or three words\n",
        "        device_keywords.append(sentence_tokens_d)\n",
        "\n",
        "  if len(keywords_patient) > 0:\n",
        "    for sent_p in keywords_patient:\n",
        "      sentence_tokens_p = modified_tokenizer(sent_p) # The modified tokenizer function is called here\n",
        "      \n",
        "      if sentence_tokens_p in keywords_corpus_patient: # To stick to two or three words\n",
        "        patient_keywords.append(sentence_tokens_p)\n",
        "  \n",
        "  if len(keywords_other) > 0:\n",
        "    for sent_o in keywords_other:\n",
        "      sentence_tokens_o = modified_tokenizer(sent_o) # The modified tokenizer function is called here\n",
        "      \n",
        "      if sentence_tokens_o in keywords_corpus_other: # To stick to two or three words\n",
        "        other_keywords.append(sentence_tokens_o)\n",
        "\n",
        "  keywords_dict[\"keywords (Device Problems)\"] = device_keywords\n",
        "  keywords_dict[\"keywords (Device Problems)_score\"] = device_score\n",
        "  keywords_dict[\"keywords (Patient Problems)\"] = patient_keywords\n",
        "  keywords_dict[\"keywords (Patient Problems)_score\"] = patient_score\n",
        "  keywords_dict[\"keywords others (interesting ones)\"] = other_keywords\n",
        "  keywords_dict[\"keywords others (interesting ones)_score\"] = other_score\n",
        "\n",
        "  return keywords_dict"
      ],
      "metadata": {
        "id": "Z6WIm-gioxz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show us the first 20 rows of the processed dataframe\n",
        "\n",
        "processed_df.head(20)"
      ],
      "metadata": {
        "id": "zv9XgY09_rNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed data frame in CSV format - change the file name not to override the unprocessed data - for the whole dataframe\n",
        "\n",
        "def save_processeddf(processed_df):\n",
        "  processed_df.to_csv(\"/content/drive/MyDrive/processed_Task_data (1) (3).csv\", encoding='utf-8', index=None)"
      ],
      "metadata": {
        "id": "xTqh_y6R_KM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the save_processeddf function\n",
        "\n",
        "save_processeddf(processed_df)"
      ],
      "metadata": {
        "id": "06j_iUSJ_y6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test script here, using the same texts for solutions 2 and 3 to see the quality of the processed data returned. This processed data \n",
        "# is expected to be used to populate the dataframe's columns requested"
      ],
      "metadata": {
        "id": "uUQxCM5boxu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = '(b)(4). this reported event and subsequent repairs were investigated through the service repair process. failure data and parts-used information were reviewed for the sap and track wise files and found relevant to the service repair. a review of the device service history record was performed from the date of manufacture to the date corresponding to this service notification number. the database showed no quality notifications were opened for the device. a review of the device history record in sap for sn (b)(4) was performed from the date of the manufacture to date of the release of product, which confirmed that this device was not involved in a production failure, and product was returned for servicing which correlates to the customer reported issue. a trackwise complaint history review was completed, and it was confirmed that there were additional complaints received with similar sn (b)(4) for the same or related failure mode. the customer stated that there was no patient involvement.'"
      ],
      "metadata": {
        "id": "CKjos5pP_IP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"the patient contacted animas alleging that the subject pump auto suspends when he wakes up in the morning. there was no mention of physical damage to the pump. there was no reported patient impact associated with this complaint. (b)(4): all buttons responded to presses normally. the keypad was removed and no damage was found to the button contacts. unrelated to the complaint, evaluation revealed the internal clock battery on the pcb board had failed. the pump would not retain the user programmed date and time settings upon removal of the primary aa battery. when a new aa battery is inserted the pump displays the default date and time which must be manually confirmed (or reset) by the user in order to proceed. the pump has not been returned to animas for evaluation. animas has conducted a review of the device history record for this pump and confirmed that it was operating within required specifications at the time of release. if the device is returned, an evaluation shall be completed and a supplemental report will be filed. no conclusions can be made at this time. (b)(4).\""
      ],
      "metadata": {
        "id": "dLhTFUP5AMnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_3 = \"it was reported that the unit was knocked over and subsequently declared multiple errors indicative of a serial peripheral interface failure. there was no patient involvement. the manufacturer's remote service technician performed troubleshooting with the customer. the customer reported that a wire had broken from the battery and the wire was soldered back together. the technician advised the customer against re-soldering components and to replace the battery if damaged. the technician also recommended that the customer replace the data acquisition (da) ribbon cable, followed by the flow sensor and central processing unit (cpu) board. date of event: (b)(6) 2020. date of report: 20apr2020.\""
      ],
      "metadata": {
        "id": "oHesWHhXAMjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_4 = \"the service report shows the customer reported that the 840 ventilator stopped cycling while in use on a pt. the pt was not harmed or injured as a result of the event. the nellcor puritan bennett customer support engineer (cse) inspected the device and could not duplicate the alleged event. the unit passed extended self-testing and no parts were replaced. it is not verified that the vent was inoperable, and that a malfunction occurred.\""
      ],
      "metadata": {
        "id": "jZtzveHZAMgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_5 = \"the customer reported that the vela ventilator alarmed low pip (peak inspiratory pressure), low ve (minute ventilation) , xdcr (transducer) fault. the customer confirmed that there was no patient involvement associated with the reported event. vyaire medical file identification: (b)(4). the customer reported that they will not return the defective pcb for evaluation. therefore, no root cause could be determined . vyaire medical will submit a supplemental report in accordance with 21 cfr section 803.56 if additional information was received.\""
      ],
      "metadata": {
        "id": "jEUkWnHiAxRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_keywordsdict_per_text(text_1)"
      ],
      "metadata": {
        "id": "o17mSmhQ_QxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55bedb62-f2f3-48ba-f566-75156d7d119b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': ['confirmed this'],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': [],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_keywordsdict_per_text(text_2)"
      ],
      "metadata": {
        "id": "18ZBkIvTAb2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfd3d12-4ac9-471e-c3b2-14e12a7ea290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': [],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': ['unrelated complaint',\n",
              "  'conclusions this time'],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_keywordsdict_per_text(text_3)"
      ],
      "metadata": {
        "id": "a5xJB6_lAoZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c081ef60-ef91-47dd-c781-89bea9df08e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': [],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': ['date report: 20apr2020'],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_keywordsdict_per_text(text_4)"
      ],
      "metadata": {
        "id": "Tuh6FjUrAbzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad469e4-16fa-4c66-dbe8-1799470f98bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': ['duplicate alleged event'],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': [],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': ['verified vent inoperable',\n",
              "  'malfunction occurred'],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_keywordsdict_per_text(text_5)"
      ],
      "metadata": {
        "id": "YR3J8FSBAbyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb33d75a-78f8-4ba0-c511-32e34fe6b655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keywords (Device Problems)': [],\n",
              " 'keywords (Device Problems)_score': [],\n",
              " 'keywords (Patient Problems)': ['involvement associated event'],\n",
              " 'keywords (Patient Problems)_score': [],\n",
              " 'keywords others (interesting ones)': ['low (minute ventilation)',\n",
              "  'xdcr (transducer) fault',\n",
              "  'root cause determined',\n",
              "  'additional information received'],\n",
              " 'keywords others (interesting ones)_score': []}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}