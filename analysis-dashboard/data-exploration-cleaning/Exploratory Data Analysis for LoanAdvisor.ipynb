{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbc790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df402ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ffc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d6862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a714fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876011a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb40339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f61458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08363106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe336e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aaea79e",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Sep 22 20:24:44 2019\n",
    "\n",
    "@author: Semiu\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "@author: Semiu: Database Systems CSci 765 Project -Data Mining Model for Predicting Loan \n",
    "Default\n",
    "Dataset can be downloaded from the supplementary material section of this journal article:\n",
    "https://www.tandfonline.com/doi/full/10.1080/10691898.2018.1434342\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Reading the United States Small Business Administration Dataset into a Dataframe\n",
    "\n",
    "Importing the Pandas library and confirming the success of the dataset reading by showing \n",
    "the first five rows of the data.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "datadf = pd.read_csv(\"SBAnational.csv\", low_memory=False)\n",
    "datadf.head(5)\n",
    "\"\"\"\n",
    "Checking if there are missing values in the dataset.\n",
    "\"\"\"\n",
    "datadf.isnull().sum()\n",
    "\"\"\"\n",
    "With the existence of missing values of categorical data types, it is important to firstly \n",
    "handle categorical data types (ordinal, class labels and nominals) before treating the missing \n",
    "values.\n",
    "a. Each of the hypothetically important features with Text datatype is checked to identify \n",
    "the nominal and ordinal types for their corresponding data handling\n",
    "b. The unique identifier is set as the index\n",
    "\"\"\"\n",
    "datadf['ApprovalDate']\n",
    "datadf2 = datadf.set_index(\"LoanNr_ChkDgt\", drop = False)\n",
    "\"\"\"\n",
    "Based on (a) above, RevLineCr, LowDoc and MIS_Status (all categorical data) are handled\n",
    "using the ordinal/nominal feature encoding methods\n",
    "\"\"\"\n",
    "class_mapping1 = {'Y': 1, 'N': 0}\n",
    "datadf2['RevLineCr'] = datadf2['RevLineCr'].map(class_mapping1)\n",
    "datadf2['RevLineCr']\n",
    "\n",
    "class_mapping2 = {'Y': 1, 'N': 0}\n",
    "datadf2['LowDoc'] = datadf2['LowDoc'].map(class_mapping2)\n",
    "datadf2['LowDoc']\n",
    "\n",
    "class_mapping3 = {'P I F': 1, 'CHGOFF': 0}\n",
    "datadf2['MIS_Status'] = datadf2['MIS_Status'].map(class_mapping3)\n",
    "datadf2['MIS_Status']\n",
    "\"\"\"\n",
    "The NaN in  RevLineCr, LowDoc and MIS_Status are replaced with 0\n",
    "\"\"\"\n",
    "datadf2['RevLineCr'].fillna(0, inplace=True)\n",
    "\n",
    "datadf2['LowDoc'].fillna(0, inplace=True)\n",
    "\n",
    "datadf2['MIS_Status'].fillna(0, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "The other missing data features are replaced with the most frequent value of their respective columns\n",
    "\"\"\"\n",
    "datadf2 = datadf2.fillna(datadf2.mode().iloc[0])\n",
    "\"\"\"\n",
    "Checking if there are still missing values in the dataset.\n",
    "\"\"\"\n",
    "datadf2.isnull().sum()\n",
    "\"\"\"\n",
    "Assign the target variable of the data frame to another variable; \"datadf2_y\"\n",
    "\"\"\"\n",
    "datadf2_y = datadf2['MIS_Status']\n",
    "\"\"\"\n",
    "Remove the target variable (MIS_Status) and other variables (LoanNr_ChkDgt, Name, City, State, Bank, Bank State, NAICS) that are intuitively \n",
    "insignifcant from  datadf2. The datadf2.columns[] is used to get the position of each column, after the last dropping, for its precise dropping. \n",
    "\"\"\"\n",
    "#The target variabble MIS_Status\n",
    "datadf2.drop(datadf2.columns[23], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble LoanNr_ChkDgt\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble Name\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble City\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble State\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble Zip\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble Bank\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble BankState\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "#The variabble NAICS\n",
    "datadf2.drop(datadf2.columns[0], axis= 1, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "The remaining columns (after all the droppings) are the predictors, thus assigned to another variable; datadf2_x\n",
    "\"\"\"\n",
    "datadf2_x = datadf2\n",
    "datadf2_x\n",
    "\"\"\"\n",
    "The columns with currency and comma are now treated to be fit for model building\n",
    "These are: DisbursementGross, BalanceGross, ChgOffPrinGr, GrApprv and SBA_Apprv  \n",
    "\"\"\"\n",
    "datadf2_x[datadf2_x.columns[13:]] = datadf2_x[datadf2_x.columns[13:]].apply(lambda x: x.str.replace('$','')).apply(lambda x: x.str.replace(',','')).astype(float)\n",
    "\"\"\"\n",
    "The datadf2_x['ChgOffDate'] is observed as misleading because its issing values ought not be filled.\n",
    "It is then dropped. It is as column [11] at this stage of data cleansing.\n",
    "\"\"\"\n",
    "datadf2_x.drop(datadf2_x.columns[11], axis= 1, inplace=True)\n",
    "\"\"\"\n",
    "The columns with date format are ApprovalDate and DisbursementDate in form of 13-Mar-10.\n",
    "These are coverted to their respective years. For example, 13-Mar-10 will become 2010.\n",
    "\"\"\"\n",
    "#Approval Date\n",
    "datadf2_x['ApprovalDate'] = pd.to_datetime(datadf2_x['ApprovalDate'], format='%d-%b-%y').dt.strftime('%Y')\n",
    " \n",
    "#DisbursementDate\n",
    "datadf2_x['DisbursementDate'] = pd.to_datetime(datadf2_x['DisbursementDate'], format='%d-%b-%y').dt.strftime('%Y')\n",
    "\n",
    "#ApprovalFY \n",
    "datadf2_x.drop(datadf2_x.columns[1], axis= 1, inplace=True) \n",
    "  \n",
    "\"\"\"\n",
    "The dataset is then split to the train and test set. The test size is set at 20%\n",
    "\"\"\"\n",
    "from sklearn.cross_validation import train_test_split\n",
    "x, y = datadf2_x.values, datadf2_y.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\"\"\"\n",
    "Training the dataset with the basic Decision Tree Classifier\n",
    "\"\"\"\n",
    "from sklearn import tree\n",
    "clf1 = tree.DecisionTreeClassifier()\n",
    "clf1 = clf1.fit(x_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Testing the accuracy of the Decision Tree Classifier\n",
    "\"\"\"\n",
    "clf1_sc = clf1.score(x_test, y_test)\n",
    "print(clf1_sc)\n",
    "\"\"\"\n",
    "Training the dataset with the Gradient Boosting Classifier\n",
    "\"\"\"\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Testing the accuracy of the Gradient Boosting Classifier\n",
    "\"\"\"\n",
    "clf2_sc = clf2.score(x_test, y_test)\n",
    "print(clf2_sc)\n",
    "\n",
    "clf2.score(x_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Comparing the Basic Decision Tree and Boosted Decision Tree with a Histogram\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "# Creating an empty Dataframe with column names as \n",
    "dfCLF = pd.DataFrame(columns=['Clf', 'Model'])\n",
    "\n",
    "# Append rows in Empty Dataframe by adding dictionaries\n",
    "dfCLF = dfCLF.append({'Clf': clf1_sc, 'Model': 'Basic Decision Tree'}, ignore_index=True)\n",
    "dfCLF = dfCLF.append({'Clf': clf2_sc, 'Model': 'Boosted Decision Tree'}, ignore_index=True)\n",
    "sns.barplot(x='Model', y='Clf', data=dfCLF)\n",
    "\n",
    "\"\"\"\n",
    "Model Performance Evaluation 1: \n",
    "Receiver Operating Characteristic (ROC) Curve and \n",
    "Area Under Curve (AUC) for the Boosted Decision Tree \n",
    "\"\"\"\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "\n",
    "probs = clf2.predict_proba(x_test)\n",
    "\n",
    "# Reading probability of second class (Pay-In-Full)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "label = 'Boosted Decision Tree Model AUC:' + ' {0:.2f}'.format(roc_auc)\n",
    "plt.plot(fpr, tpr, c = 'g', label = label, linewidth = 4)\n",
    "plt.xlabel('False Positive Rate', fontsize = 14)\n",
    "plt.ylabel('True Positive Rate', fontsize = 14)\n",
    "plt.title('Receiver Operating Characteristic', fontsize = 14)\n",
    "plt.legend(loc = 'lower right', fontsize = 14)\n",
    "\n",
    "\"\"\"\n",
    "Model Performance Evaluation 2: \n",
    "Cumulative Accuracy Profile (CAP) Curve for the Boosted Decision Tree \n",
    "\"\"\"\n",
    "#total number of y_test data instances\n",
    "total_y_test = len(y_test)\n",
    "#total number of class 1.0 (PIF)\n",
    "class_1_count = np.sum(y_test)\n",
    "\n",
    "print (class_1_count) \n",
    "#Total number of class 0.0 (CHGOFF)\n",
    "class_0_count = total_y_test - class_1_count\n",
    "\n",
    "print (class_0_count) \n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "#Random Model -suggesting that detection of class 1.0 will grow linearly.\n",
    "plt.plot([0, total_y_test], [0, class_1_count], c = 'r', linestyle = '--', label = 'Random Model')\n",
    "\n",
    "#Perfect Model detecting the class 1.0 data instances\n",
    "plt.plot([0, class_1_count, total_y_test],[0, class_1_count, class_1_count], c = 'grey', linewidth = 2, label = 'Perfect Model')\n",
    "\n",
    "#Boosted Decision Tree Model\n",
    "model_y = [y for _, y in sorted(zip(probs, y_test), reverse = True)]\n",
    "y_values = np.append([0], np.cumsum(model_y))\n",
    "x_values = np.arange(0, total_y_test + 1)\n",
    "plt.plot(x_values, y_values, c = 'b', label = 'Boosted Decision Tree', linewidth = 4)\n",
    "\n",
    "plt.xlabel('Total Observations', fontsize = 14)\n",
    "plt.ylabel('Class 1.0 Observations', fontsize = 14)\n",
    "plt.title('Cumulative Accuracy Profile (CAP)', fontsize = 14)\n",
    "plt.legend(loc = 'lower right', fontsize = 14)\n",
    "\n",
    "\"\"\"\n",
    "Model Performance Evaluation 3: \n",
    "Cumulative Accuracy Profile (CAP) Analysis Using AUC and Plot for the Boosted Decision Tree\n",
    "(with class_1_observed) \n",
    "\"\"\"\n",
    "# Area under Random Model\n",
    "a = auc([0, total_y_test], [0, class_1_count])\n",
    "\n",
    "# Area between Perfect and Random Model\n",
    "aP = auc([0, class_1_count, total_y_test], [0, class_1_count, class_1_count]) - a\n",
    "\n",
    "# Area between Trained and Random Model\n",
    "aR = auc(x_values, y_values) - a\n",
    "\n",
    "#Accuracy Rate for Boosted Decision Tree: 0.9959399421174234\n",
    "print(\"Accuracy Rate for Boosted Decision Tree with class 1: {}\".format(aR / aP))\n",
    "\n",
    "#Using Plot with CAP\n",
    "index = int((50*total_y_test / 100))\n",
    "\n",
    "## 50% Verticcal line from x-axis\n",
    "plt.plot([index, index], [0, y_values[index]], c ='g', linestyle = '--')\n",
    "\n",
    "## Horizontal line to y-axis from prediction model\n",
    "plt.plot([0, index], [y_values[index], y_values[index]], c = 'b', linestyle = '--')\n",
    "\n",
    "class_1_observed = y_values[index] * 100 / max(y_values)\n",
    "\n",
    "print('The percentage of class_1_observed is : ', class_1_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2d0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8273679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba489c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422796d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55393c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d70d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426793f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e84504d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f0d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubleSize(a,b):\n",
    "    for i in a:\n",
    "        if i == b:\n",
    "            b *=2\n",
    "            i=b\n",
    "            continue\n",
    "        else:\n",
    "            i = i\n",
    "            continue\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73adc3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleSize([1,2,4,11,12,8],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ff3edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956b618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
